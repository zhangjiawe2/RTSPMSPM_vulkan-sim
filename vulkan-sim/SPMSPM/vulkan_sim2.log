MESA: Linking to LLVM resource bind packing.
Getting ray tracing pipeline properties...
Vertices: 6 Indices: 6 values: 2
LVP: vkBuffer size 104 created at 0x7ffff396daf8
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11b9e4c2e0; llvmpipe_resource 0x5d11b9e4c2e0; pipe_memory_allocation 0x5d11b9e4d000;
LVP: Binding lvp_buffer 0x5d11b9e4c260: buffer->bo: 0x5d11b9e4c2e0; buffer->pmem: 0x5d11b9e4d000; memory-offset: 0;


        *** GPGPU-Sim Simulator Version 4.0.0 - Vulkan-Sim version 2.0.0  [build gpgpu-sim_git-commit-dc742faa6e5fcfaf415fe51eab71dcf9345388fd_modified_0.0] ***


GPGPU-Sim PTX: simulation mode 1818648434 (can change with PTX_SIM_MODE_FUNC environment variable:
               1=functional simulation only, 0=detailed performance simulator)
GPGPU-Sim PTX: overriding embedded ptx with ptx file (PTX_SIM_USE_PTX_FILE is set)
GPGPU-Sim: Configuration options:

-save_embedded_ptx                      0 # saves ptx files embedded in binary as <n>.ptx
-keep                                   0 # keep intermediate files created by GPGPU-Sim when interfacing with external programs
-gpgpu_ptx_save_converted_ptxplus                    0 # Saved converted ptxplus to a file
-gpgpu_occupancy_sm_number                   75 # The SM number to pass to ptxas when getting register usage for computing GPU occupancy. This parameter is required in the config.
-ptx_opcode_latency_int      4,13,4,5,145,32 # Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,19,25,145,32
-ptx_opcode_latency_fp          4,13,4,5,39 # Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,30
-ptx_opcode_latency_dp         8,19,8,8,330 # Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,335
-ptx_opcode_latency_sfu                  100 # Opcode latencies for SFU instructionsDefault 8
-ptx_opcode_latency_tesnor                   64 # Opcode latencies for Tensor instructionsDefault 64
-ptx_opcode_initiation_int          2,2,2,2,8,4 # Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,4,4,32,4
-ptx_opcode_initiation_fp            2,2,2,2,4 # Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,5
-ptx_opcode_initiation_dp          4,4,4,4,130 # Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,130
-ptx_opcode_initiation_sfu                    8 # Opcode initiation intervals for sfu instructionsDefault 8
-ptx_opcode_initiation_tensor                   64 # Opcode initiation intervals for tensor instructionsDefault 64
-cdp_latency         7200,8000,100,12000,1600 # CDP API latency <cudaStreamCreateWithFlags, cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>Default 7200,8000,100,12000,1600
-network_mode                           2 # Interconnection network mode
-inter_config_file                   mesh # Interconnection network config file
-icnt_in_buffer_limit                  512 # in_buffer_limit
-icnt_out_buffer_limit                  512 # out_buffer_limit
-icnt_subnets                           2 # subnets
-icnt_arbiter_algo                      1 # arbiter_algo
-icnt_verbose                           0 # inct_verbose
-icnt_grant_cycles                      1 # grant_cycles
-gpgpu_ptx_use_cuobjdump                    1 # Use cuobjdump to extract ptx and sass from binaries
-gpgpu_experimental_lib_support                    0 # Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]
-checkpoint_option                      0 #  checkpointing flag (0 = no checkpoint)
-checkpoint_kernel                      1 #  checkpointing during execution of which kernel (1- 1st kernel)
-checkpoint_CTA                         0 #  checkpointing after # of CTA (< less than total CTA)
-resume_option                          0 #  resume flag (0 = no resume)
-resume_kernel                          0 #  Resume from which kernel (1= 1st kernel)
-resume_CTA                             0 #  resume from which CTA 
-checkpoint_CTA_t                       0 #  resume from which CTA 
-checkpoint_insn_Y                      0 #  resume from which CTA 
-gpgpu_ptx_convert_to_ptxplus                    0 # Convert SASS (native ISA) to ptxplus and run ptxplus
-gpgpu_ptx_force_max_capability                   75 # Force maximum compute capability
-gpgpu_ptx_inst_debug_to_file                    0 # Dump executed instructions' debug information to file
-gpgpu_ptx_inst_debug_file       inst_debug.txt # Executed instructions' debug output file
-gpgpu_ptx_inst_debug_thread_uid                    1 # Thread UID for executed instructions' debug output
-gpgpu_simd_model                       1 # 1 = post-dominator
-gpgpu_simd_rec_time_out                   -1 # -1 = no reconvergence time out
-gpgpu_simd_rec_size                    6 # number of physical entries in the reconvergence table
-gpgpu_simd_st_size                     6 # number of physical entries in the splits table
-gpgpu_simd_rec_replacement                    0 # reconvergence table replacement policy
-gpgpu_simd_st_replacement                    0 # splits table replacement policy
-gpgpu_shader_core_pipeline              1024:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>}
-gpgpu_tex_cache:l1  N:4:128:256,L:R:m:N:L,T:512:8,128:2 # per-shader L1 texture cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}
-gpgpu_const_cache:l1 N:128:64:8,L:R:f:N:L,S:2:64,4 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_rt_cache:l1   S:32:128:16,L:R:m:L:P,A:256:256,32:0,32 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_rt_use_l1                        1 # use existing L1 cache instead of dedicated L0 cache 
-gpgpu_rt_perfect_mem                    0 # assume 0 latency BVH accesses 
-gpgpu_rt_coherence_engine                    0 # enable coherency engine (ray sorting) 
-gpgpu_rt_coherence_engine_config   100,10,4,g,0,3,2,0 # max cycles, hash 
-gpgpu_rt_disable_rt_cache                    0 # bypass RT cache and connect RT unit directly to interconnect 
-gpgpu_rt_max_warps                     4 # max number of warps concurrently in one rt core 
-gpgpu_rt_max_mshr                     64 # max number of MSHR entries in RT unit 
-gpgpu_rt_coalesce_warps                    0 # try to coalesce memory requests between warps 
-gpgpu_rt_intersection_latency        4,8,8,4,8,8,8 # latency of pipelined intersection tests (7 types)
-gpgpu_rt_intersection_table_type                    0 # type of intersection table
-gpgpu_cache:il1     N:64:128:16,L:R:f:N:L,S:2:48,4 # shader L1 instruction cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:dl1     S:1:128:512,L:L:s:N:L,A:256:8,16:0,32 # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_l1_banks                         4 # The number of L1 cache banks
-gpgpu_l1_banks_byte_interleaving                   32 # l1 banks byte interleaving granularity
-gpgpu_l1_banks_hashing_function                    0 # l1 banks hashing function
-gpgpu_l1_latency                      20 # L1 Hit Latency
-gpgpu_smem_latency                    20 # smem Latency
-gpgpu_cache:dl1PrefL1                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_cache:dl1PrefShared                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_gmem_skip_L1D                    0 # global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-n_regfile_gating_group                    4 # group of lanes that should be read/written together)
-gpgpu_clock_gated_reg_file                    0 # enable clock gated reg file for power calculations
-gpgpu_clock_gated_lanes                    0 # enable clock gated lanes for power calculations
-gpgpu_shader_registers                65536 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_registers_per_block                65536 # Maximum number of registers per CTA. (default 8192)
-gpgpu_ignore_resources_limitation                    0 # gpgpu_ignore_resources_limitation (default 0)
-gpgpu_shader_cta                      32 # Maximum number of concurrent CTAs in shader (default 8)
-gpgpu_num_cta_barriers                   16 # Maximum number of named barriers per CTA (default 16)
-gpgpu_n_clusters                      30 # number of processing clusters
-gpgpu_n_cores_per_cluster                    1 # number of simd cores per cluster
-gpgpu_n_cluster_ejection_buffer_size                   32 # number of packets in ejection buffer
-gpgpu_n_ldst_response_buffer_size                    2 # number of response packets in ld/st unit ejection buffer
-gpgpu_shmem_per_block                65536 # Size of shared memory per thread block or CTA (default 48kB)
-gpgpu_shmem_size                   65536 # Size of shared memory per shader core (default 16kB)
-gpgpu_adaptive_cache_config                    0 # adaptive_cache_config
-gpgpu_shmem_sizeDefault                65536 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefL1                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefShared                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_num_banks                   32 # Number of banks in the shared memory in each shader core (default 16)
-gpgpu_shmem_limited_broadcast                    0 # Limit shared memory to do one broadcast per cycle (default on)
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_mem_unit_ports                    1 # The number of memory transactions allowed per core cycle
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_warp_issue_shader                    0 # Specify which shader core to collect the warp issue distribution from
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = enabled)
-gpgpu_num_reg_banks                   16 # Number of register banks (default = 8)
-gpgpu_reg_bank_use_warp_id                    0 # Use warp ID in mapping registers to banks (default = off)
-gpgpu_sub_core_model                    1 # Sub Core Volta/Pascal model (default = off)
-gpgpu_enable_specialized_operand_collector                    0 # enable_specialized_operand_collector
-gpgpu_operand_collector_num_units_sp                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_dp                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_sfu                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_int                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_tensor_core                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_mem                    2 # number of collector units (default = 2)
-gpgpu_operand_collector_num_units_gen                    8 # number of collector units (default = 0)
-gpgpu_operand_collector_num_in_ports_sp                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_gen                    8 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sp                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_gen                    8 # number of collector unit in ports (default = 0)
-gpgpu_coalesce_arch                   75 # Coalescing arch (GT200 = 13, Fermi = 20)
-gpgpu_num_sched_per_core                    4 # Number of warp schedulers per core
-gpgpu_max_insn_issue_per_warp                    1 # Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)
-gpgpu_dual_issue_diff_exec_units                    1 # should dual issue use two different execution unit resources (Default = 1)
-gpgpu_simt_core_sim_order                    1 # Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)
-gpgpu_pipeline_widths 4,0,4,4,4,4,0,4,4,4,8,4,4,4,4 # Pipeline widths ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE,ID_OC_RT,OC_EX_RT
-gpgpu_tensor_core_avail                    1 # Tensor Core Available (default=0)
-gpgpu_num_sp_units                     4 # Number of SP units (default=1)
-gpgpu_num_dp_units                     0 # Number of DP units (default=0)
-gpgpu_num_int_units                    4 # Number of INT units (default=0)
-gpgpu_num_sfu_units                    4 # Number of SF units (default=1)
-gpgpu_num_tensor_core_units                    4 # Number of tensor_core units (default=1)
-gpgpu_num_rt_core_units                    1 # Number of rt core units (default=1)
-gpgpu_num_mem_units                    1 # Number if ldst units (default=1) WARNING: not hooked up to anything
-gpgpu_scheduler                      gto # Scheduler configuration: < lrr | gto | two_level_active > If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>For complete list of prioritization values see shader.h enum scheduler_prioritization_typeDefault: gto
-gpgpu_concurrent_kernel_sm                    0 # Support concurrent kernels on a SM (default = disabled)
-gpgpu_perfect_inst_const_cache                    1 # perfect inst and const cache mode, so all inst and const hits in the cache(default = disabled)
-gpgpu_inst_fetch_throughput                    4 # the number of fetched intruction per warp each cycle
-gpgpu_reg_file_port_throughput                    2 # the number ports of the register file
-specialized_unit_1         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_2         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_3         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_4         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_5         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_6         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_7         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_8         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-gpgpu_perf_sim_memcpy                    1 # Fill the L2 cache on memcpy
-gpgpu_simple_dram_model                    0 # simple_dram_model with fixed latency and BW
-gpgpu_dram_scheduler                    1 # 0 = fifo, 1 = FR-FCFS (defaul)
-gpgpu_dram_partition_queues          64:64:64:64 # i2$:$2d:d2$:$2i
-l2_ideal                               0 # Use a ideal L2 cache that always hit
-gpgpu_cache:dl2     S:64:128:16,L:B:m:L:P,A:192:4,32:0,32 # unified banked L2 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}
-gpgpu_cache:dl2_texture_only                    0 # L2 cache used for texture only
-gpgpu_n_mem                           12 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_sub_partition_per_mchannel                    2 # number of memory subpartition in each memory module
-gpgpu_n_mem_per_ctrlr                    1 # number of memory chips per memory controller
-gpgpu_memlatency_stat                   14 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_frfcfs_dram_sched_queue_size                   64 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_return_queue_size                  192 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    2 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                   16 # Burst length of each DRAM request (default = 4 data bus cycle)
-dram_data_command_freq_ratio                    4 # Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)
-gpgpu_dram_timing_opt nbk=16:CCD=4:RRD=10:RCD=20:RAS=50:RP=20:RC=62: CL=20:WL=8:CDLR=9:WR=20:nbkgrp=4:CCDL=4:RTPL=4 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}
-gpgpu_l2_rop_latency                  160 # ROP queue latency (default 85)
-dram_latency                         100 # DRAM latency (default 30)
-dram_dual_bus_interface                    0 # dual_bus_interface (default = 0) 
-dram_bnk_indexing_policy                    0 # dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)
-dram_bnkgrp_indexing_policy                    1 # dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)
-dram_seperate_write_queue_enable                    0 # Seperate_Write_Queue_Enable
-dram_write_queue_size             32:28:16 # Write_Queue_Size
-dram_elimnate_rw_turnaround                    0 # elimnate_rw_turnaround i.e set tWTR and tRTW = 0
-icnt_flit_size                        40 # icnt_flit_size
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-gpgpu_mem_addr_test                    0 # run sweep test to check address mapping for aliased address
-gpgpu_mem_address_mask                    1 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_memory_partition_indexing                    0 # 0 = no indexing, 1 = bitwise xoring, 2 = IPoly, 3 = custom indexing
-gpuwattch_xml_file         gpuwattch.xml # GPUWattch XML file
-power_simulation_enabled                    0 # Turn on power simulator (1=On, 0=Off)
-power_per_cycle_dump                    0 # Dump detailed power output each cycle
-power_trace_enabled                    1 # produce a file for the power trace (1=On, 0=Off)
-power_trace_zlevel                     6 # Compression level of the power trace output log (0=no comp, 9=highest)
-steady_power_levels_enabled                    1 # produce a file for the steady power levels (1=On, 0=Off)
-steady_state_definition                  8:4 # allowed deviation:number of samples
-gpgpu_intermittent_stats                    0 # print intermittent stats
-gpgpu_intermittent_stats_freq                10000 # intermittent stats frequency
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_cta                          0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_completed_cta                    0 # terminates gpu simulation early (0 = no limit)
-gpgpu_runtime_stat                   500 # display runtime statistics such as dram utilization {<freq>:<flag>}
-liveness_message_freq                    1 # Minimum number of seconds between simulation liveness messages (0 = always print)
-gpgpu_compute_capability_major                    7 # Major compute capability version number
-gpgpu_compute_capability_minor                    5 # Minor compute capability version number
-gpgpu_flush_l1_cache                    1 # Flush L1 cache at the end of each kernel call
-gpgpu_flush_l2_cache                    0 # Flush L2 cache at the end of each kernel call
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_ptx_instruction_classification                    1 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-gpgpu_clock_domains 1365.0:1365.0:1365.0:3500.0 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_max_concurrent_kernel                    8 # maximum kernels that can run concurrently on GPU
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-visualizer_enabled                     0 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile     Characterization # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-gpgpu_stack_size_limit                 1024 # GPU thread stack size
-gpgpu_heap_size_limit              8388608 # GPU malloc heap size 
-gpgpu_runtime_sync_depth_limit                    2 # GPU device runtime synchronize depth
-gpgpu_runtime_pending_launch_count_limit                 2048 # GPU device runtime pending launch count
-trace_enabled                          0 # Turn on traces
-trace_components                    none # comma seperated list of traces to enable. Complete list found in trace_streams.tup. Default none
-trace_sampling_core                    0 # The core which is printed using CORE_DPRINTF. Default 0
-trace_sampling_memory_partition                   -1 # The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
-gpgpu_kernel_launch_latency                 5000 # Kernel launch latency in cycles. Default: 0
-gpgpu_cdp_enabled                      0 # Turn on CDP
-gpgpu_TB_launch_latency                    0 # thread block launch latency in cycles. Default: 0
-gpgpu_max_simulated_rt_kernels                    2 # Max simulated kernels, used to limit how many frames we render. Default: 0
Streaming cache with 8192 mshr
GPGPU-Sim: Ray Coherence Engine Settings:
	Enabled: No
	Max cycles: 100
	Max packets: 4
	Sorting hash: grid-spherical
	Bits: 3-2
DRAM Timing Options:
nbk                                    16 # number of banks
CCD                                     4 # column to column delay
RRD                                    10 # minimal delay between activation of rows in different banks
RCD                                    20 # row to column delay
RAS                                    50 # time needed to activate row
RP                                     20 # time needed to precharge (deactivate) row
RC                                     62 # row cycle time
CDLR                                    9 # switching from write to read (changes tWTR)
WR                                     20 # last data-in to row precharge
CL                                     20 # CAS latency
WL                                      8 # Write latency
nbkgrp                                  4 # number of bank groups
CCDL                                    4 # column to column delay between accesses to different bank groups
RTPL                                    4 # read to precharge delay between accesses to different bank groups
Total number of memory sub partition = 24
addr_dec_mask[CHIP]  = 0000000000000000 	high:64 low:0
addr_dec_mask[BK]    = 0000000000007080 	high:15 low:7
addr_dec_mask[ROW]   = 000000000fff8000 	high:28 low:15
addr_dec_mask[COL]   = 0000000000000f7f 	high:12 low:0
addr_dec_mask[BURST] = 000000000000001f 	high:5 low:0
sub_partition_id_mask = 0000000000000080
GPGPU-Sim uArch: clock freqs: 1365000000.000000:1365000000.000000:1365000000.000000:3500000000.000000
GPGPU-Sim uArch: clock periods: 0.00000000073260073260:0.00000000073260073260:0.00000000073260073260:0.00000000028571428571
*** Initializing Memory Statistics ***
GPGPU-Sim uArch: performance model initialization complete.
gpgpusim: binding gpgpusim buffer 0xc0000000 (size 104) to vulkan buffer 0x5d11b9e4d000
LVP: gpgpusim buffer size 104 allocated at 0xc0000000
LVP: Finding mapped memory for mem 0x5d11b9e4c4f0: mem->pmem 0x5d11b9e4d000 
LVP: Identified at 0x5d11b9e4d000
Vertex Buffer Device Address: 0x5d11b9e4d000
SUCCESS: Vulkan-sim + Mesa device address works
Ray Count : 2
LVP: vkBuffer size 32 created at 0x7ffff396db10
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc5eb0; llvmpipe_resource 0x5d11c3fc5eb0; pipe_memory_allocation 0x5d11c3fc7000;
LVP: Binding lvp_buffer 0x5d11c3fc5e30: buffer->bo: 0x5d11c3fc5eb0; buffer->pmem: 0x5d11c3fc7000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000100 (size 32) to vulkan buffer 0x5d11c3fc7000
LVP: gpgpusim buffer size 32 allocated at 0xc0000100
LVP: Finding mapped memory for mem 0x5d11c3fc60c0: mem->pmem 0x5d11c3fc7000 
LVP: Identified at 0x5d11c3fc7000
Ray Buffer Device Address: 0x5d11b9e4d000
LVP: vkBuffer size 100 created at 0x7ffff396db28
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc6200; llvmpipe_resource 0x5d11c3fc6200; pipe_memory_allocation 0x5d11c3fc8000;
LVP: Binding lvp_buffer 0x5d11c3fc6180: buffer->bo: 0x5d11c3fc6200; buffer->pmem: 0x5d11c3fc8000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000200 (size 100) to vulkan buffer 0x5d11c3fc8000
LVP: gpgpusim buffer size 100 allocated at 0xc0000200
lvp_GetAccelerationStructureBuildSizesKHR
LVP: Size of BVH structure is 0x1c0
LVP: vkBuffer size 448 created at 0x7ffff396db38
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc66d0; llvmpipe_resource 0x5d11c3fc66d0; pipe_memory_allocation 0x5d11c3fc9000;
LVP: Binding lvp_buffer 0x5d11c3fc6650: buffer->bo: 0x5d11c3fc66d0; buffer->pmem: 0x5d11c3fc9000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000300 (size 448) to vulkan buffer 0x5d11c3fc9000
LVP: gpgpusim buffer size 448 allocated at 0xc0000300
lvp_CreateAccelerationStructureKHR
LVP: Accel structure (size 0x1c0) created from lvp_buffer (size 0x1c0). 
LVP: Buffer 0x5d11c3fc9000 + 0x0 = 0x5d11c3fc9000 allocated to accel structure 0x5d11ba0ef510
gpgpusim: set BLAS address for 0x1c0 at 0x5d11c3fc9000 to 0xc0000300
LVP: vkBuffer size 160 created at 0x7ffff396db50
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc72e0; llvmpipe_resource 0x5d11c3fc72e0; pipe_memory_allocation 0x5d11c3fca000;
LVP: Binding lvp_buffer 0x5d11c3fc7260: buffer->bo: 0x5d11c3fc72e0; buffer->pmem: 0x5d11c3fca000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000500 (size 160) to vulkan buffer 0x5d11c3fca000
LVP: gpgpusim buffer size 160 allocated at 0xc0000500
LVP: Building acceleration structure
gpgpusim: set geometry
EMBREE: 2 triangles added; total: 2
EMBREE: New rtcBVH structure created at 0x5d11c3fc8530
EMBREE: Building 6 wide tree (max depth 32) using quality 1
EMBREE: Build BVH with root 0x5d11c3fc8880
Compressed Node: i	exp: (2, 0, 1)	origin: (0.500000, 1.000000, -0.500000)
Children:
	0x5d11c3fc88e8: (0, 0, 0), (64, 128, 128)
	0x5d11c3fc88f4: (64, 0, 0), (128, 128, 128)
	(nil): (0, 0, 0), (0, 0, 0)
	(nil): (0, 0, 0), (0, 0, 0)
	(nil): (0, 0, 0), (0, 0, 0)
	(nil): (0, 0, 0), (0, 0, 0)
EMBREE: Set dst_map 0x5d11c3fc9000 = accel->address.bo 0x5d11c3fc9000 + accel->address.offset 0x0 for accel 0x5d11ba0ef510
EMBREE: Pack bvh 0x7ffff396d930 to dst_map 0x5d11c3fc9000
EMBREE: Pack root 0x5d11c3fc8880 to root_map 0x5d11c3fc9040
EMBREE: Release rtcBVH structure created at 0x5d11c3fc8530
LVP: Command Buffer:
lvp_GetAccelerationStructureDeviceAddressKHR
LVP: Returning BLAS 0x5d11ba0ef510 address 0x5d11c3fc9000
LVP: vkBuffer size 64 created at 0x7ffff396db78
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc81b0; llvmpipe_resource 0x5d11c3fc81b0; pipe_memory_allocation 0x5d11c3fcc000;
LVP: Binding lvp_buffer 0x5d11c3fc9f80: buffer->bo: 0x5d11c3fc81b0; buffer->pmem: 0x5d11c3fcc000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000600 (size 64) to vulkan buffer 0x5d11c3fcc000
LVP: gpgpusim buffer size 64 allocated at 0xc0000600
LVP: Finding mapped memory for mem 0x5d11c3fc83c0: mem->pmem 0x5d11c3fcc000 
LVP: Identified at 0x5d11c3fcc000
lvp_GetAccelerationStructureBuildSizesKHR
LVP: Size of BVH structure is 0x1c0
LVP: vkBuffer size 448 created at 0x7ffff396db90
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fcbae0; llvmpipe_resource 0x5d11c3fcbae0; pipe_memory_allocation 0x5d11c3fcd000;
LVP: Binding lvp_buffer 0x5d11c3fc8480: buffer->bo: 0x5d11c3fcbae0; buffer->pmem: 0x5d11c3fcd000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000700 (size 448) to vulkan buffer 0x5d11c3fcd000
LVP: gpgpusim buffer size 448 allocated at 0xc0000700
lvp_CreateAccelerationStructureKHR
LVP: Accel structure (size 0x1c0) created from lvp_buffer (size 0x1c0). 
LVP: Buffer 0x5d11c3fcd000 + 0x0 = 0x5d11c3fcd000 allocated to accel structure 0x5d11ba0ef3c0
gpgpusim: set TLAS address 0x5d11c3fcd000 to 0xc0000700
LVP: vkBuffer size 92 created at 0x7ffff396dba8
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc78d0; llvmpipe_resource 0x5d11c3fc78d0; pipe_memory_allocation 0x5d11c3fce000;
LVP: Binding lvp_buffer 0x5d11c3fcd1d0: buffer->bo: 0x5d11c3fc78d0; buffer->pmem: 0x5d11c3fce000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000900 (size 92) to vulkan buffer 0x5d11c3fce000
LVP: gpgpusim buffer size 92 allocated at 0xc0000900
LVP: Building acceleration structure
gpgpusim: set geometry
EMBREE: Get accel instance -> device 0x7ffff396d5f0
EMBREE: Add AABB geometry: geomID: 0, primID 0, box (0.500, 1.000, -0.500), (2.500, 1.500, 0.500)
EMBREE: New rtcBVH structure created at 0x5d11c3fc8530
EMBREE: Set dst_map 0x5d11c3fcd000 = accel->address.bo 0x5d11c3fcd000 + accel->address.offset 0x0 for accel 0x5d11ba0ef3c0
EMBREE: Pack bvh 0x7ffff396d930 to dst_map 0x5d11c3fcd000
EMBREE: Get accel instance -> device 0x7ffff396c420
EMBREE: Pack root 0x7ffff396d9d0 to root_map 0x5d11c3fcd040
gpgpusim: set AS
EMBREE: Release rtcBVH structure created at 0x5d11c3fc8530
LVP: Command Buffer:
*****************TLAS built successfully!***********
LVP: vkBuffer size 8 created at 0x7ffff396dbc0
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc8df0; llvmpipe_resource 0x5d11c3fc8df0; pipe_memory_allocation 0x5d11c3fcf000;
LVP: Binding lvp_buffer 0x5d11c3fc8cc0: buffer->bo: 0x5d11c3fc8df0; buffer->pmem: 0x5d11c3fcf000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000a00 (size 8) to vulkan buffer 0x5d11c3fcf000
LVP: gpgpusim buffer size 8 allocated at 0xc0000a00
LVP: Finding mapped memory for mem 0x5d11c3fcd250: mem->pmem 0x5d11c3fcf000 
LVP: Identified at 0x5d11c3fcf000
LVP: vkBuffer size 12 created at 0x7ffff396dbd8
LLVM: llvmpipe_resource_bind_backing: pipe_screen 0x5d11b9e485b0; pipe_resource 0x5d11c3fc7b60; llvmpipe_resource 0x5d11c3fc7b60; pipe_memory_allocation 0x5d11c3fd0000;
LVP: Binding lvp_buffer 0x5d11c3fc8820: buffer->bo: 0x5d11c3fc7b60; buffer->pmem: 0x5d11c3fd0000; memory-offset: 0;
gpgpusim: binding gpgpusim buffer 0xc0000b00 (size 12) to vulkan buffer 0x5d11c3fd0000
LVP: gpgpusim buffer size 12 allocated at 0xc0000b00
LVP: Finding mapped memory for mem 0x5d11c3fc7d70: mem->pmem 0x5d11c3fd0000 
LVP: Identified at 0x5d11c3fd0000
LVP: Create descriptor set layout...
LVP: Set Layout with 6 bindings...
DESCRIPTOR TYPE SET: 1000150000
LVP: Set Layout: type VK_DESCRIPTOR_TYPE_ACCELERATION_STRUCTURE_KHR; stage flag 0x100
DESCRIPTOR TYPE SET: 7
LVP: Set Layout: type VK_DESCRIPTOR_TYPE_STORAGE_BUFFER; stage flag 0x100
DESCRIPTOR TYPE SET: 7
LVP: Set Layout: type VK_DESCRIPTOR_TYPE_STORAGE_BUFFER; stage flag 0x500
DESCRIPTOR TYPE SET: 7
LVP: Set Layout: type VK_DESCRIPTOR_TYPE_STORAGE_BUFFER; stage flag 0x500
DESCRIPTOR TYPE SET: 7
LVP: Set Layout: type VK_DESCRIPTOR_TYPE_STORAGE_BUFFER; stage flag 0x500
DESCRIPTOR TYPE SET: 7
LVP: Set Layout: type VK_DESCRIPTOR_TYPE_STORAGE_BUFFER; stage flag 0x500
LVP: Create pipeline layout...
LVP: Create descriptor pool...
LVP: Allocate descriptor sets...
LVP: Creating descriptor sets with 6 bindings at 0x5d11c3fc88a0...
LVP: Update descriptor sets...
DESC AT: 0x5d11c3fc88f8 with type 1000150000
LVP: Setting lvp_acceleration_structure 0x5d11ba0ef3c0 for descriptor 0 (0). info.ubo: address 0x5d11c3fcd000 + offset 0 = 0x5d11c3fcd000 
DESC AT: 0x5d11c3fc8928 with type 7
LVP: Setting lvp_buffer 0x5d11c3fc5e30 for descriptor 1 (0). info.ssbo: buffer 0x5d11c3fc5eb0 + offset 0 = 0x5d11c3fc5eb0; stored at 0x5d11c3fc7000 using pointer 0xc0000100 with 32 bytes. 
LVP: gpgpusim buffer at 0xc0000100
DESC AT: 0x5d11c3fc8958 with type 7
LVP: Setting lvp_buffer 0x5d11c3fc6180 for descriptor 2 (0). info.ssbo: buffer 0x5d11c3fc6200 + offset 0 = 0x5d11c3fc6200; stored at 0x5d11c3fc8000 using pointer 0xc0000200 with 100 bytes. 
LVP: gpgpusim buffer at 0xc0000200
DESC AT: 0x5d11c3fc8988 with type 7
LVP: Setting lvp_buffer 0x5d11b9e4c260 for descriptor 3 (0). info.ssbo: buffer 0x5d11b9e4c2e0 + offset 0 = 0x5d11b9e4c2e0; stored at 0x5d11b9e4d000 using pointer 0xc0000000 with 104 bytes. 
LVP: gpgpusim buffer at 0xc0000000
DESC AT: 0x5d11c3fc89b8 with type 7
LVP: Setting lvp_buffer 0x5d11c3fc8cc0 for descriptor 4 (0). info.ssbo: buffer 0x5d11c3fc8df0 + offset 0 = 0x5d11c3fc8df0; stored at 0x5d11c3fcf000 using pointer 0xc0000a00 with 8 bytes. 
LVP: gpgpusim buffer at 0xc0000a00
DESC AT: 0x5d11c3fc89e8 with type 7
LVP: Setting lvp_buffer 0x5d11c3fc8820 for descriptor 5 (0). info.ssbo: buffer 0x5d11c3fc7b60 + offset 0 = 0x5d11c3fc7b60; stored at 0x5d11c3fd0000 using pointer 0xc0000b00 with 12 bytes. 
LVP: gpgpusim buffer at 0xc0000b00
*******************success build shaderModule********
*******************success build shaderModule********
*******************success build shaderModule********
*******************success build shaderModule********
Full line: 	// Untranslated deref_atomic instruction. %ssa_39, %ssa_37, %ssa_38, 1, 9;	// vec1 32 ssa_39 = intrinsic deref_atomic (%ssa_37, %ssa_38) (1, 9) /* access=1 */ /* atomic_op=9 */

Line #: 132
Command: 
Inst. class: InstructionClass.Untranslated
Incomplete line
LVP: Creating ray tracing pipeline...
LVP: Compiling ray tracing pipeline...
LVP: Compiling shader stage 0
LVP: Translating shader 0 (type 8)
GPGPU-SIM VULKAN: Translating NIR MESA_SHADER_RAYGEN to PTX
LVP: Compiling shader stage 1
LVP: Translating shader 1 (type 11)
GPGPU-SIM VULKAN: Translating NIR MESA_SHADER_MISS to PTX
LVP: Compiling shader stage 2
LVP: Translating shader 2 (type 9)
GPGPU-SIM VULKAN: Translating NIR MESA_SHADER_ANY_HIT to PTX
LVP: Compiling shader stage 3
LVP: Translating shader 3 (type 10)
GPGPU-SIM VULKAN: Translating NIR MESA_SHADER_CLOSEST_HIT to PTX
LVP: run_rt_translation_passes
MESA: ERROR ** while translating nir to PTX 256
GPGPU-Sim: *** exit detected ***
